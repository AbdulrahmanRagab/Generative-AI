{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92b91aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a42aad44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY: value\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "from openai import OpenAI\n",
    "import langchain\n",
    "import os\n",
    "# Step 1: Load environment variables\n",
    "%reload_ext dotenv\n",
    "%dotenv\n",
    "# Step 3: Access your OpenAI API key from environment variables\n",
    "for key, value in os.environ.items():\n",
    "    if key == \"OPENAI_API_KEY\":\n",
    "        print(f\"{key}: value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cae41e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa69424c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "chat = ChatOpenAI(model='gpt-4o-mini', temperature=0,max_completion_tokens=50)\n",
    "# response = chat.invoke(\"I have recently adopted a dog. Could you suggest some dog names?\")\n",
    "# print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea378054",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_core.prompts import (ChatPromptTemplate, \n",
    "                                    HumanMessagePromptTemplate, \n",
    "                                    MessagesPlaceholder,\n",
    "                                    PromptTemplate)\n",
    "\n",
    "from langchain.chains.llm import LLMChain\n",
    "\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain.memory import (ConversationBufferMemory,\n",
    "                              ConversationBufferWindowMemory,\n",
    "                              ConversationSummaryMemory,\n",
    "                              CombinedMemory)\n",
    "\n",
    "from langchain.globals import set_verbose\n",
    "set_verbose(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27d93e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE = '''\n",
    "The following is a friendly conversation between a human and an AI. \n",
    "If the AI does not know the answer to a question, it truthfully says it does not know.\n",
    "\n",
    "Past messages:\n",
    "{message_buffer_log}\n",
    "\n",
    "Conversation summary:\n",
    "{message_summary_log}\n",
    "\n",
    "Human: {question}\n",
    "AI:\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e040e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate.from_template(template=TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abdelrahman\\AppData\\Local\\Temp\\ipykernel_21764\\1048753577.py:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  chat_buffer_memory = ConversationBufferMemory(memory_key='message_buffer_log',\n",
      "C:\\Users\\Abdelrahman\\AppData\\Local\\Temp\\ipykernel_21764\\1048753577.py:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  chat_summary_memory = ConversationSummaryMemory(llm=chat, memory_key='message_summary_log',\n"
     ]
    }
   ],
   "source": [
    "chat_buffer_memory = ConversationBufferMemory(memory_key='message_buffer_log',\n",
    "                                              input_key='question',return_messages=False)\n",
    "\n",
    "chat_summary_memory = ConversationSummaryMemory(llm=chat, memory_key='message_summary_log',\n",
    "                                                input_key='question', return_messages=False)\n",
    "\n",
    "chat_combined_memory = CombinedMemory(memories=[chat_buffer_memory,chat_summary_memory])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b05e0c28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message_buffer_log': '', 'message_summary_log': ''}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_combined_memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abdelrahman\\AppData\\Local\\Temp\\ipykernel_21764\\2929776012.py:1: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(llm=chat,prompt=prompt_template,memory=chat_combined_memory)\n"
     ]
    }
   ],
   "source": [
    "chain = LLMChain(llm=chat,prompt=prompt_template,memory=chat_combined_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "290861f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "The following is a friendly conversation between a human and an AI. \n",
      "If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Past messages:\n",
      "\n",
      "\n",
      "Conversation summary:\n",
      "\n",
      "\n",
      "Human: Can you give me an interesting fact I probably didn't know about?\n",
      "AI:\n",
      "\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mProgressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\n",
      "\n",
      "EXAMPLE\n",
      "Current summary:\n",
      "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\n",
      "\n",
      "New lines of conversation:\n",
      "Human: Why do you think artificial intelligence is a force for good?\n",
      "AI: Because artificial intelligence will help humans reach their full potential.\n",
      "\n",
      "New summary:\n",
      "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\n",
      "END OF EXAMPLE\n",
      "\n",
      "Current summary:\n",
      "\n",
      "\n",
      "New lines of conversation:\n",
      "Human: Can you give me an interesting fact I probably didn't know about?\n",
      "AI: Did you know that honey never spoils? Archaeologists have found pots of honey in ancient Egyptian tombs that are over 3,000 years old and still perfectly edible!\n",
      "\n",
      "New summary:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': \"Can you give me an interesting fact I probably didn't know about?\",\n",
       " 'message_buffer_log': '',\n",
       " 'message_summary_log': '',\n",
       " 'text': 'Did you know that honey never spoils? Archaeologists have found pots of honey in ancient Egyptian tombs that are over 3,000 years old and still perfectly edible!'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reponse = chain.invoke({'question' : \"Can you give me an interesting fact I probably didn't know about?\"})\n",
    "reponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5469faea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did you know that honey never spoils? Archaeologists have found pots of honey in ancient Egyptian tombs that are over 3,000 years old and still perfectly edible!\n"
     ]
    }
   ],
   "source": [
    "print(reponse['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "The following is a friendly conversation between a human and an AI. \n",
      "If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Past messages:\n",
      "Human: Can you give me an interesting fact I probably didn't know about?\n",
      "AI: Did you know that honey never spoils? Archaeologists have found pots of honey in ancient Egyptian tombs that are over 3,000 years old and still perfectly edible!\n",
      "\n",
      "Conversation summary:\n",
      "The human asks the AI for an interesting fact. The AI shares that honey never spoils, citing that archaeologists have found pots of honey in ancient Egyptian tombs that are over 3,000 years old and still perfectly edible.\n",
      "\n",
      "Human: Can you give me another interesting fact I probably didn't know about?\n",
      "AI:\n",
      "\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mProgressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\n",
      "\n",
      "EXAMPLE\n",
      "Current summary:\n",
      "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\n",
      "\n",
      "New lines of conversation:\n",
      "Human: Why do you think artificial intelligence is a force for good?\n",
      "AI: Because artificial intelligence will help humans reach their full potential.\n",
      "\n",
      "New summary:\n",
      "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\n",
      "END OF EXAMPLE\n",
      "\n",
      "Current summary:\n",
      "The human asks the AI for an interesting fact. The AI shares that honey never spoils, citing that archaeologists have found pots of honey in ancient Egyptian tombs that are over 3,000 years old and still perfectly edible.\n",
      "\n",
      "New lines of conversation:\n",
      "Human: Can you give me another interesting fact I probably didn't know about?\n",
      "AI: Did you know that octopuses have three hearts? Two pump blood to the gills, where it gets oxygen, and the third pumps it to the rest of the body. When they swim, the heart that delivers blood to the body actually stops\n",
      "\n",
      "New summary:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': \"Can you give me another interesting fact I probably didn't know about?\",\n",
       " 'message_buffer_log': \"Human: Can you give me an interesting fact I probably didn't know about?\\nAI: Did you know that honey never spoils? Archaeologists have found pots of honey in ancient Egyptian tombs that are over 3,000 years old and still perfectly edible!\",\n",
       " 'message_summary_log': 'The human asks the AI for an interesting fact. The AI shares that honey never spoils, citing that archaeologists have found pots of honey in ancient Egyptian tombs that are over 3,000 years old and still perfectly edible.',\n",
       " 'text': 'Did you know that octopuses have three hearts? Two pump blood to the gills, where it gets oxygen, and the third pumps it to the rest of the body. When they swim, the heart that delivers blood to the body actually stops'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reponse = chain.invoke({'question' : \"Can you give me another interesting fact I probably didn't know about?\"})\n",
    "reponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e7b2f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did you know that octopuses have three hearts? Two pump blood to the gills, where it gets oxygen, and the third pumps it to the rest of the body. When they swim, the heart that delivers blood to the body actually stops\n"
     ]
    }
   ],
   "source": [
    "print(reponse['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4ab8a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "The following is a friendly conversation between a human and an AI. \n",
      "If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Past messages:\n",
      "Human: Can you give me an interesting fact I probably didn't know about?\n",
      "AI: Did you know that honey never spoils? Archaeologists have found pots of honey in ancient Egyptian tombs that are over 3,000 years old and still perfectly edible!\n",
      "Human: Can you give me another interesting fact I probably didn't know about?\n",
      "AI: Did you know that octopuses have three hearts? Two pump blood to the gills, where it gets oxygen, and the third pumps it to the rest of the body. When they swim, the heart that delivers blood to the body actually stops\n",
      "\n",
      "Conversation summary:\n",
      "The human asks the AI for an interesting fact. The AI shares that honey never spoils, citing that archaeologists have found pots of honey in ancient Egyptian tombs that are over 3,000 years old and still perfectly edible. The human then\n",
      "\n",
      "Human: Can you talk about Rodri?\n",
      "AI:\n",
      "\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mProgressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\n",
      "\n",
      "EXAMPLE\n",
      "Current summary:\n",
      "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\n",
      "\n",
      "New lines of conversation:\n",
      "Human: Why do you think artificial intelligence is a force for good?\n",
      "AI: Because artificial intelligence will help humans reach their full potential.\n",
      "\n",
      "New summary:\n",
      "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\n",
      "END OF EXAMPLE\n",
      "\n",
      "Current summary:\n",
      "The human asks the AI for an interesting fact. The AI shares that honey never spoils, citing that archaeologists have found pots of honey in ancient Egyptian tombs that are over 3,000 years old and still perfectly edible. The human then\n",
      "\n",
      "New lines of conversation:\n",
      "Human: Can you talk about Rodri?\n",
      "AI: Rodri, whose full name is Rodrigo Hernández Cascante, is a Spanish professional footballer who plays as a defensive midfielder. He is known for his excellent passing ability, tactical awareness, and physical presence on the field. Rodri began his career at\n",
      "\n",
      "New summary:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'Can you talk about Rodri?',\n",
       " 'message_buffer_log': \"Human: Can you give me an interesting fact I probably didn't know about?\\nAI: Did you know that honey never spoils? Archaeologists have found pots of honey in ancient Egyptian tombs that are over 3,000 years old and still perfectly edible!\\nHuman: Can you give me another interesting fact I probably didn't know about?\\nAI: Did you know that octopuses have three hearts? Two pump blood to the gills, where it gets oxygen, and the third pumps it to the rest of the body. When they swim, the heart that delivers blood to the body actually stops\",\n",
       " 'message_summary_log': 'The human asks the AI for an interesting fact. The AI shares that honey never spoils, citing that archaeologists have found pots of honey in ancient Egyptian tombs that are over 3,000 years old and still perfectly edible. The human then',\n",
       " 'text': 'Rodri, whose full name is Rodrigo Hernández Cascante, is a Spanish professional footballer who plays as a defensive midfielder. He is known for his excellent passing ability, tactical awareness, and physical presence on the field. Rodri began his career at'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reponse = chain.invoke({'question' : \"Can you talk about Rodri?\"})\n",
    "reponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca0a9403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rodri, whose full name is Rodrigo Hernández Cascante, is a Spanish professional footballer who plays as a defensive midfielder. He is known for his excellent passing ability, tactical awareness, and physical presence on the field. Rodri began his career at\n"
     ]
    }
   ],
   "source": [
    "print(reponse['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "08d95995",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message_buffer_log': \"Human: Can you give me an interesting fact I probably didn't know about?\\nAI: Did you know that honey never spoils? Archaeologists have found pots of honey in ancient Egyptian tombs that are over 3,000 years old and still perfectly edible!\\nHuman: Can you give me another interesting fact I probably didn't know about?\\nAI: Did you know that octopuses have three hearts? Two pump blood to the gills, where it gets oxygen, and the third pumps it to the rest of the body. When they swim, the heart that delivers blood to the body actually stops\\nHuman: Can you talk about Rodri?\\nAI: Rodri, whose full name is Rodrigo Hernández Cascante, is a Spanish professional footballer who plays as a defensive midfielder. He is known for his excellent passing ability, tactical awareness, and physical presence on the field. Rodri began his career at\"}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_buffer_memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message_summary_log': 'The human asks the AI for an interesting fact, and the AI shares that honey never spoils, noting that archaeologists have found pots of honey in ancient Egyptian tombs that are over 3,000 years old and still perfectly edible. The human'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_summary_memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message_buffer_log': \"Human: Can you give me an interesting fact I probably didn't know about?\\nAI: Did you know that honey never spoils? Archaeologists have found pots of honey in ancient Egyptian tombs that are over 3,000 years old and still perfectly edible!\\nHuman: Can you give me another interesting fact I probably didn't know about?\\nAI: Did you know that octopuses have three hearts? Two pump blood to the gills, where it gets oxygen, and the third pumps it to the rest of the body. When they swim, the heart that delivers blood to the body actually stops\\nHuman: Can you talk about Rodri?\\nAI: Rodri, whose full name is Rodrigo Hernández Cascante, is a Spanish professional footballer who plays as a defensive midfielder. He is known for his excellent passing ability, tactical awareness, and physical presence on the field. Rodri began his career at\",\n",
       " 'message_summary_log': 'The human asks the AI for an interesting fact, and the AI shares that honey never spoils, noting that archaeologists have found pots of honey in ancient Egyptian tombs that are over 3,000 years old and still perfectly edible. The human'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_combined_memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "381eb5b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The human asks the AI for an interesting fact, and the AI shares that honey never spoils, noting that archaeologists have found pots of honey in ancient Egyptian tombs that are over 3,000 years old and still perfectly edible. The human\n"
     ]
    }
   ],
   "source": [
    "print(chat_combined_memory.load_memory_variables({})['message_summary_log'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
