{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dba21e0",
   "metadata": {},
   "source": [
    "### Conversation Summary Memory \n",
    "الكود بيبني نظام محادثة بين إنسان وذكاء اصطناعي، الذكاء الصناعي بيحتفظ `بملخص` المحادثة باستخدام ConversationSummaryMemory علشان يتذكر الحوار، بدل ما يخزن كل الكلام كامل. وكمان بيستخدم LLMChain عشان يربط الـ model بالـ prompt والذاكرة مع بعض"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "33fde96c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY: value\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "from openai import OpenAI\n",
    "import langchain\n",
    "import os\n",
    "# Step 1: Load environment variables\n",
    "%reload_ext dotenv\n",
    "%dotenv\n",
    "# Step 3: Access your OpenAI API key from environment variables\n",
    "for key, value in os.environ.items():\n",
    "    if key == \"OPENAI_API_KEY\":\n",
    "        print(f\"{key}: value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4e8f6e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e6181aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "chat = ChatOpenAI(model='gpt-4o-mini', temperature=0,max_completion_tokens=50)\n",
    "# response = chat.invoke(\"I have recently adopted a dog. Could you suggest some dog names?\")\n",
    "# print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a3aa270f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_core.prompts import (ChatPromptTemplate, \n",
    "                                    HumanMessagePromptTemplate, \n",
    "                                    MessagesPlaceholder,\n",
    "                                    PromptTemplate)\n",
    "\n",
    "from langchain.chains.llm import LLMChain\n",
    "\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain.memory import ConversationBufferMemory,ConversationBufferWindowMemory,ConversationSummaryMemory\n",
    "\n",
    "from langchain.globals import set_verbose\n",
    "set_verbose(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE = '''\n",
    "The following is a friendly conversation between a human and an AI.  \n",
    "If the AI does not know the answer to a question, it truthfully says it does not know.\n",
    "\n",
    "Current conversation:\n",
    "{message_log}\n",
    "\n",
    "Human:\n",
    "{question}\n",
    "\n",
    "AI:\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b89936",
   "metadata": {},
   "source": [
    "الـ message_log هو عبارة عن ملخص للمحادثة اللي حصلت بين الإنسان والذكاء الصناعي.\n",
    "الـ ConversationSummaryMemory بيبني الملخص ده بعد كل سؤال وجواب، وبيخزنه عشان لما تيجي تبعت سؤال جديد، يتحط جوه {message_log} في الـ TEMPLAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate.from_template(template=TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ca380f11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConversationSummaryMemory(llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000002859567D000>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000002859567FD60>, root_client=<openai.OpenAI object at 0x000002859566EF80>, root_async_client=<openai.AsyncOpenAI object at 0x000002859567D060>, model_name='gpt-4o-mini', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********'), max_tokens=50), chat_memory=InMemoryChatMessageHistory(messages=[]), memory_key='message_log')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_memory = ConversationSummaryMemory(llm=chat,\n",
    "                                        memory_key='message_log',\n",
    "                                        return_messages=False)\n",
    "chat_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "44378e01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message_log': ''}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = LLMChain(llm=chat,prompt=prompt_template, memory=chat_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "789d46b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "The following is a friendly conversation between a human and an AI.  \n",
      "If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "\n",
      "Human:\n",
      "Can you give me an interesting fact I probably didn't know about?\n",
      "\n",
      "AI:\n",
      "\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mProgressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\n",
      "\n",
      "EXAMPLE\n",
      "Current summary:\n",
      "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\n",
      "\n",
      "New lines of conversation:\n",
      "Human: Why do you think artificial intelligence is a force for good?\n",
      "AI: Because artificial intelligence will help humans reach their full potential.\n",
      "\n",
      "New summary:\n",
      "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\n",
      "END OF EXAMPLE\n",
      "\n",
      "Current summary:\n",
      "\n",
      "\n",
      "New lines of conversation:\n",
      "Human: Can you give me an interesting fact I probably didn't know about?\n",
      "AI: Sure! Did you know that honey never spoils? Archaeologists have found pots of honey in ancient Egyptian tombs that are over 3,000 years old and still perfectly edible!\n",
      "\n",
      "New summary:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': \"Can you give me an interesting fact I probably didn't know about?\",\n",
       " 'message_log': '',\n",
       " 'text': 'Sure! Did you know that honey never spoils? Archaeologists have found pots of honey in ancient Egyptian tombs that are over 3,000 years old and still perfectly edible!'}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response =  chain.invoke({'question':\"Can you give me an interesting fact I probably didn't know about?\"})\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Did you know that honey never spoils? Archaeologists have found pots of honey in ancient Egyptian tombs that are over 3,000 years old and still perfectly edible!\n"
     ]
    }
   ],
   "source": [
    "print(response['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2ba1e026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "The following is a friendly conversation between a human and an AI.  \n",
      "If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "The human asks the AI for an interesting fact. The AI shares that honey never spoils, citing that archaeologists have found pots of honey in ancient Egyptian tombs that are over 3,000 years old and still perfectly edible.\n",
      "\n",
      "Human:\n",
      "Can you talk about rodri?\n",
      "\n",
      "AI:\n",
      "\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mProgressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\n",
      "\n",
      "EXAMPLE\n",
      "Current summary:\n",
      "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\n",
      "\n",
      "New lines of conversation:\n",
      "Human: Why do you think artificial intelligence is a force for good?\n",
      "AI: Because artificial intelligence will help humans reach their full potential.\n",
      "\n",
      "New summary:\n",
      "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\n",
      "END OF EXAMPLE\n",
      "\n",
      "Current summary:\n",
      "The human asks the AI for an interesting fact. The AI shares that honey never spoils, citing that archaeologists have found pots of honey in ancient Egyptian tombs that are over 3,000 years old and still perfectly edible.\n",
      "\n",
      "New lines of conversation:\n",
      "Human: Can you talk about rodri?\n",
      "AI: Sure! Rodri, whose full name is Rodrigo Hernández Cascante, is a Spanish professional footballer who plays as a defensive midfielder. He is known for his strong tackling, passing ability, and tactical awareness. Rodri began his career at Villarreal\n",
      "\n",
      "New summary:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'Can you talk about rodri?',\n",
       " 'message_log': 'The human asks the AI for an interesting fact. The AI shares that honey never spoils, citing that archaeologists have found pots of honey in ancient Egyptian tombs that are over 3,000 years old and still perfectly edible.',\n",
       " 'text': 'Sure! Rodri, whose full name is Rodrigo Hernández Cascante, is a Spanish professional footballer who plays as a defensive midfielder. He is known for his strong tackling, passing ability, and tactical awareness. Rodri began his career at Villarreal'}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response =  chain.invoke({'question':\"Can you talk about rodri?\"})\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "The following is a friendly conversation between a human and an AI.  \n",
      "If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "The human asks the AI for an interesting fact, and the AI shares that honey never spoils, noting that archaeologists have found pots of honey in ancient Egyptian tombs that are over 3,000 years old and still perfectly edible. The human\n",
      "\n",
      "Human:\n",
      "Can you talk about Kevin debroyne?\n",
      "\n",
      "AI:\n",
      "\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mProgressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\n",
      "\n",
      "EXAMPLE\n",
      "Current summary:\n",
      "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\n",
      "\n",
      "New lines of conversation:\n",
      "Human: Why do you think artificial intelligence is a force for good?\n",
      "AI: Because artificial intelligence will help humans reach their full potential.\n",
      "\n",
      "New summary:\n",
      "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\n",
      "END OF EXAMPLE\n",
      "\n",
      "Current summary:\n",
      "The human asks the AI for an interesting fact, and the AI shares that honey never spoils, noting that archaeologists have found pots of honey in ancient Egyptian tombs that are over 3,000 years old and still perfectly edible. The human\n",
      "\n",
      "New lines of conversation:\n",
      "Human: Can you talk about Kevin debroyne?\n",
      "AI: Kevin De Bruyne is a Belgian professional footballer who plays as a midfielder for Manchester City in the English Premier League and the Belgium national team. Known for his exceptional passing, vision, and ability to score goals, De Bruyne is often regarded as\n",
      "\n",
      "New summary:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'Can you talk about Kevin debroyne?',\n",
       " 'message_log': 'The human asks the AI for an interesting fact, and the AI shares that honey never spoils, noting that archaeologists have found pots of honey in ancient Egyptian tombs that are over 3,000 years old and still perfectly edible. The human',\n",
       " 'text': 'Kevin De Bruyne is a Belgian professional footballer who plays as a midfielder for Manchester City in the English Premier League and the Belgium national team. Known for his exceptional passing, vision, and ability to score goals, De Bruyne is often regarded as'}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response =  chain.invoke({'question':\"Can you talk about Kevin debroyne?\"})\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "801ded27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message_log': 'The human asks the AI for an interesting fact, and the AI shares that honey never spoils, noting that archaeologists have found pots of honey in ancient Egyptian tombs that are over 3,000 years old and still perfectly edible. The human'}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba03998",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
