{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dba21e0",
   "metadata": {},
   "source": [
    "### Conversation Summary Memory \n",
    "الكود بيبني نظام محادثة بين إنسان وذكاء اصطناعي، الذكاء الصناعي بيحتفظ `بملخص` المحادثة باستخدام ConversationSummaryMemory علشان يتذكر الحوار، بدل ما يخزن كل الكلام كامل. وكمان بيستخدم LLMChain عشان يربط الـ model بالـ prompt والذاكرة مع بعض"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "33fde96c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY: value\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "from openai import OpenAI\n",
    "import langchain\n",
    "import os\n",
    "# Step 1: Load environment variables\n",
    "%reload_ext dotenv\n",
    "%dotenv\n",
    "# Step 3: Access your OpenAI API key from environment variables\n",
    "for key, value in os.environ.items():\n",
    "    if key == \"OPENAI_API_KEY\":\n",
    "        print(f\"{key}: value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4e8f6e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e6181aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "chat = ChatOpenAI(model='gpt-4o-mini', temperature=0,max_completion_tokens=50)\n",
    "# response = chat.invoke(\"I have recently adopted a dog. Could you suggest some dog names?\")\n",
    "# print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a3aa270f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_core.prompts import (ChatPromptTemplate, \n",
    "                                    HumanMessagePromptTemplate, \n",
    "                                    MessagesPlaceholder,\n",
    "                                    PromptTemplate)\n",
    "\n",
    "from langchain.chains.llm import LLMChain\n",
    "\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain.memory import ConversationBufferMemory,ConversationBufferWindowMemory,ConversationSummaryMemory\n",
    "\n",
    "from langchain.globals import set_verbose\n",
    "set_verbose(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb77ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder, HumanMessagePromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2779d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the system message and human message template\n",
    "sys_message = SystemMessage(content='''The following is a friendly conversation between a human and an AI.  \n",
    "If the AI does not know the answer to a question, it truthfully says it does not know.''')\n",
    "\n",
    "hum_message_template = HumanMessagePromptTemplate.from_template(template='{question}')\n",
    "\n",
    "message_history = MessagesPlaceholder(variable_name='message_log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6551e7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the chat prompt template\n",
    "chat_template = ChatPromptTemplate.from_messages([\n",
    "    sys_message,\n",
    "    message_history,\n",
    "    hum_message_template\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68215d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize memory (using ConversationSummaryMemory as in your original)\n",
    "chat_memory = ConversationSummaryMemory(\n",
    "    llm=chat,\n",
    "    memory_key='message_log',\n",
    "    return_messages=True  # Changed to True to work with MessagesPlaceholder\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77356e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the chain\n",
    "chain = LLMChain(\n",
    "    llm=chat,\n",
    "    prompt=chat_template,\n",
    "    memory=chat_memory,\n",
    "    verbose=True  # Optional for debugging\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d889dda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "response = chain.invoke({'question': \"Can you give me an interesting fact I probably didn't know about?\"})\n",
    "print(response['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066af904",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.invoke({'question': \"Can you talk about rodri?\"})\n",
    "print(response['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1ddf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.invoke({'question': \"Can you talk about Kevin debroyne?\"})\n",
    "print(response['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502d3175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check memory state\n",
    "print(chat_memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
