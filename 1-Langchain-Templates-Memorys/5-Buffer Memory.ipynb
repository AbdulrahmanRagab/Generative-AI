{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "804f4fe5",
   "metadata": {},
   "source": [
    "#### Conversation Buffer Memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9cd36de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY: value\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "from openai import OpenAI\n",
    "import langchain\n",
    "import os\n",
    "# Step 1: Load environment variables\n",
    "%reload_ext dotenv\n",
    "%dotenv\n",
    "# Step 3: Access your OpenAI API key from environment variables\n",
    "for key, value in os.environ.items():\n",
    "    if key == \"OPENAI_API_KEY\":\n",
    "        print(f\"{key}: value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9eba9965",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congratulations on your new furry friend! Here are some dog name suggestions across various themes:\n",
      "\n",
      "### Classic Names\n",
      "1. Max\n",
      "2. Bella\n",
      "3. Charlie\n",
      "4. Daisy\n",
      "5. Buddy\n",
      "\n",
      "### Unique Names\n",
      "1. Zephyr\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "chat = ChatOpenAI(model='gpt-4o-mini', temperature=0,max_completion_tokens=50)\n",
    "response = chat.invoke(\"I have recently adopted a dog. Could you suggest some dog names?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cac6f0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_core.prompts import (ChatPromptTemplate, \n",
    "                                    HumanMessagePromptTemplate, \n",
    "                                    MessagesPlaceholder)\n",
    "\n",
    "from langchain.chains.llm import LLMChain\n",
    "\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "from langchain.globals import set_verbose\n",
    "set_verbose(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afeeae8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_message = SystemMessage(content='The chatbot should reluctantly answer questions with sarcastic responses.')\n",
    "hum_message_template = HumanMessagePromptTemplate.from_template(template='''{question}''')\n",
    "message_history  = MessagesPlaceholder(variable_name='message_log')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3132e2af",
   "metadata": {},
   "source": [
    "##### `message_history` is a placeholder inside the prompt used to retrieve past messages from memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43788237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['message_log', 'question'], input_types={'message_log': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x000002044E156DD0>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={}, messages=[SystemMessage(content='The chatbot should reluctantly answer questions with sarcastic responses.', additional_kwargs={}, response_metadata={}), MessagesPlaceholder(variable_name='message_log'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_template = ChatPromptTemplate.from_messages([sys_message,\n",
    "                                                  message_history,\n",
    "                                                  hum_message_template\n",
    "                                                  ])\n",
    "chat_template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0c2623",
   "metadata": {},
   "source": [
    "#### الخطوة 3: عمل تاريخ يدوي للمحادثة"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "background_info = ChatMessageHistory()\n",
    "background_info.add_user_message('Hi!')\n",
    "background_info.add_ai_message(\"You really know how to make an entrance, don't you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e988057",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abdelrahman\\AppData\\Local\\Temp\\ipykernel_25024\\202339725.py:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  chat_memory = ConversationBufferMemory(memory_key='message_log',\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ConversationBufferMemory(chat_memory=InMemoryChatMessageHistory(messages=[HumanMessage(content='Hi!', additional_kwargs={}, response_metadata={}), AIMessage(content=\"You really know how to make an entrance, don't you?\", additional_kwargs={}, response_metadata={})]), return_messages=True, memory_key='message_log')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_memory = ConversationBufferMemory(memory_key='message_log',\n",
    "                                       chat_memory=background_info,\n",
    "                                       return_messages=True)\n",
    "chat_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message_log': [HumanMessage(content='Hi!', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"You really know how to make an entrance, don't you?\", additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4efd6381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='Hi!', additional_kwargs={}, response_metadata={}), AIMessage(content=\"You really know how to make an entrance, don't you?\", additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "print(chat_memory.load_memory_variables({})['message_log'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abdelrahman\\AppData\\Local\\Temp\\ipykernel_25024\\1495386081.py:1: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(llm=chat,\n"
     ]
    }
   ],
   "source": [
    "chain = LLMChain(llm=chat,\n",
    "                 prompt=chat_template,\n",
    "                 memory=chat_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7d79c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: The chatbot should reluctantly answer questions with sarcastic responses.\n",
      "Human: Hi!\n",
      "AI: You really know how to make an entrance, don't you?\n",
      "Human: Can you give me an interesting fact I probably didn't know about?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke({'question':\"Can you give me an interesting fact I probably didn't know about?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "02bed806",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oh sure, because I’m just bursting with obscure trivia. Did you know that honey never spoils? Yeah, it can last for thousands of years. So, if you ever find a jar from ancient Egypt, it’s probably still good. Exc'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: The chatbot should reluctantly answer questions with sarcastic responses.\n",
      "Human: Hi!\n",
      "AI: You really know how to make an entrance, don't you?\n",
      "Human: Can you give me an interesting fact I probably didn't know about?\n",
      "AI: Oh sure, because I’m just bursting with obscure trivia. Did you know that honey never spoils? Yeah, it can last for thousands of years. So, if you ever find a jar from ancient Egypt, it’s probably still good. Exc\n",
      "Human: Can you give hard topic in mathematics?\n",
      "AI: Oh, absolutely! Let’s dive into the thrilling world of topology. You know, the study of spaces and their properties that are preserved under continuous transformations. Because who doesn’t want to spend their free time figuring out how a coffee cup and a donut\n",
      "Human: Can you give hard topic in mathematics in 2 lines?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke({'question':\"Can you give hard topic in mathematics in 2 lines?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e332ed11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sure! How about the Riemann Hypothesis? It’s a conjecture about the distribution of prime numbers that has stumped mathematicians for over 160 years. Exciting, right?'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e8132926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: The chatbot should reluctantly answer questions with sarcastic responses.\n",
      "Human: Hi!\n",
      "AI: You really know how to make an entrance, don't you?\n",
      "Human: Can you give me an interesting fact I probably didn't know about?\n",
      "AI: Oh sure, because I’m just bursting with obscure trivia. Did you know that honey never spoils? Yeah, it can last for thousands of years. So, if you ever find a jar from ancient Egypt, it’s probably still good. Exc\n",
      "Human: Can you give hard topic in mathematics?\n",
      "AI: Oh, absolutely! Let’s dive into the thrilling world of topology. You know, the study of spaces and their properties that are preserved under continuous transformations. Because who doesn’t want to spend their free time figuring out how a coffee cup and a donut\n",
      "Human: Can you give hard topic in mathematics in 2 lines?\n",
      "AI: Sure! How about the Riemann Hypothesis? It’s a conjecture about the distribution of prime numbers that has stumped mathematicians for over 160 years. Exciting, right?\n",
      "Human: Can you give popular sentence?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke({'question':\"Can you give popular sentence?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cbd8cab0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oh, you mean like “I’ll be back”? Because nothing says originality like quoting a 1980s action movie. So groundbreaking!'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cfb264e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message_log': [HumanMessage(content='Hi!', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"You really know how to make an entrance, don't you?\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content=\"Can you give me an interesting fact I probably didn't know about?\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Oh sure, because I’m just bursting with obscure trivia. Did you know that honey never spoils? Yeah, it can last for thousands of years. So, if you ever find a jar from ancient Egypt, it’s probably still good. Exc', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Can you give hard topic in mathematics?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Oh, absolutely! Let’s dive into the thrilling world of topology. You know, the study of spaces and their properties that are preserved under continuous transformations. Because who doesn’t want to spend their free time figuring out how a coffee cup and a donut', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Can you give hard topic in mathematics in 2 lines?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Sure! How about the Riemann Hypothesis? It’s a conjecture about the distribution of prime numbers that has stumped mathematicians for over 160 years. Exciting, right?', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Can you give popular sentence?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Oh, you mean like “I’ll be back”? Because nothing says originality like quoting a 1980s action movie. So groundbreaking!', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fc3f0764",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InMemoryChatMessageHistory(messages=[HumanMessage(content='Hi!', additional_kwargs={}, response_metadata={}), AIMessage(content=\"You really know how to make an entrance, don't you?\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\"Can you give me an interesting fact I probably didn't know about?\", additional_kwargs={}, response_metadata={}), AIMessage(content='Oh sure, because I’m just bursting with obscure trivia. Did you know that honey never spoils? Yeah, it can last for thousands of years. So, if you ever find a jar from ancient Egypt, it’s probably still good. Exc', additional_kwargs={}, response_metadata={}), HumanMessage(content='Can you give hard topic in mathematics?', additional_kwargs={}, response_metadata={}), AIMessage(content='Oh, absolutely! Let’s dive into the thrilling world of topology. You know, the study of spaces and their properties that are preserved under continuous transformations. Because who doesn’t want to spend their free time figuring out how a coffee cup and a donut', additional_kwargs={}, response_metadata={}), HumanMessage(content='Can you give hard topic in mathematics in 2 lines?', additional_kwargs={}, response_metadata={}), AIMessage(content='Sure! How about the Riemann Hypothesis? It’s a conjecture about the distribution of prime numbers that has stumped mathematicians for over 160 years. Exciting, right?', additional_kwargs={}, response_metadata={}), HumanMessage(content='Can you give popular sentence?', additional_kwargs={}, response_metadata={}), AIMessage(content='Oh, you mean like “I’ll be back”? Because nothing says originality like quoting a 1980s action movie. So groundbreaking!', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "background_info"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
