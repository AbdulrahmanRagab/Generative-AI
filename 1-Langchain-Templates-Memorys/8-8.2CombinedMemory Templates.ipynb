{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ef8904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY: value\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "from openai import OpenAI\n",
    "import langchain\n",
    "import os\n",
    "# Step 1: Load environment variables\n",
    "%reload_ext dotenv\n",
    "%dotenv\n",
    "# Step 3: Access your OpenAI API key from environment variables\n",
    "for key, value in os.environ.items():\n",
    "    if key == \"OPENAI_API_KEY\":\n",
    "        print(f\"{key}: value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8954cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb1328b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "chat = ChatOpenAI(model='gpt-4o-mini', temperature=0,max_completion_tokens=50)\n",
    "# response = chat.invoke(\"I have recently adopted a dog. Could you suggest some dog names?\")\n",
    "# print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ef8cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_core.prompts import (ChatPromptTemplate, \n",
    "                                    HumanMessagePromptTemplate, \n",
    "                                    MessagesPlaceholder,\n",
    "                                    PromptTemplate)\n",
    "\n",
    "from langchain.chains.llm import LLMChain\n",
    "\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain.memory import (ConversationBufferMemory,\n",
    "                              ConversationBufferWindowMemory,\n",
    "                              ConversationSummaryMemory,\n",
    "                              CombinedMemory)\n",
    "\n",
    "from langchain.globals import set_verbose\n",
    "set_verbose(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a286fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder, HumanMessagePromptTemplate\n",
    "\n",
    "# Define the system message\n",
    "sys_message = SystemMessage(content='''The following is a friendly conversation between a human and an AI. \n",
    "If the AI does not know the answer to a question, it truthfully says it does not know.''')\n",
    "\n",
    "# Define message placeholders for both buffer and summary\n",
    "buffer_placeholder = MessagesPlaceholder(variable_name='message_buffer_log')\n",
    "summary_placeholder = MessagesPlaceholder(variable_name='message_summary_log') \n",
    "hum_message_template = HumanMessagePromptTemplate.from_template(template='{question}')\n",
    "\n",
    "# Create the chat prompt template\n",
    "chat_template = ChatPromptTemplate.from_messages([\n",
    "    sys_message,\n",
    "    buffer_placeholder,\n",
    "    summary_placeholder,\n",
    "    hum_message_template\n",
    "])\n",
    "\n",
    "# Initialize memories (note return_messages=True for both)\n",
    "chat_buffer_memory = ConversationBufferMemory(\n",
    "    memory_key='message_buffer_log',\n",
    "    input_key='question',\n",
    "    return_messages=True  # Changed to True for MessagesPlaceholder\n",
    ")\n",
    "\n",
    "chat_summary_memory = ConversationSummaryMemory(\n",
    "    llm=chat,\n",
    "    memory_key='message_summary_log',\n",
    "    input_key='question',\n",
    "    return_messages=True  # Changed to True for MessagesPlaceholder\n",
    ")\n",
    "\n",
    "chat_combined_memory = CombinedMemory(memories=[chat_buffer_memory, chat_summary_memory])\n",
    "\n",
    "# Create the chain\n",
    "chain = LLMChain(\n",
    "    llm=chat,\n",
    "    prompt=chat_template,\n",
    "    memory=chat_combined_memory,\n",
    "    verbose=True  # Optional for debugging\n",
    ")\n",
    "\n",
    "# Example usage\n",
    "response = chain.invoke({'question': \"Can you give me an interesting fact I probably didn't know about?\"})\n",
    "print(response['text'])\n",
    "\n",
    "response = chain.invoke({'question': \"Can you give me another interesting fact I probably didn't know about?\"})\n",
    "print(response['text'])\n",
    "\n",
    "response = chain.invoke({'question': \"Can you talk about Rodri?\"})\n",
    "print(response['text'])\n",
    "\n",
    "# Check memory states\n",
    "print(\"\\nBuffer Memory:\")\n",
    "print(chat_buffer_memory.load_memory_variables({}))\n",
    "\n",
    "print(\"\\nSummary Memory:\")\n",
    "print(chat_summary_memory.load_memory_variables({}))\n",
    "\n",
    "print(\"\\nCombined Memory:\")\n",
    "combined_memory = chat_combined_memory.load_memory_variables({})\n",
    "print(combined_memory)\n",
    "print(\"\\nSummary from Combined Memory:\")\n",
    "print(combined_memory['message_summary_log'])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
